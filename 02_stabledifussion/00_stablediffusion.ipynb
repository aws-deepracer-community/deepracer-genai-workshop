{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dab554c6",
   "metadata": {},
   "source": [
    "# Modify DeepRacer track images using Stable Diffusion and analyze learning pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd530481-aafe-4765-9575-576d04c13c1e",
   "metadata": {},
   "source": [
    "\n",
    "The objective of our lab will be to modify the simulated DeepRacer track images to add more real world features. We will then validate what the car is looking at by passing these images to a pre-trained model and observing the heatmap trend. To achieve this, we will deploy Stable diffusion models to perform image2image generation. We will be using Stable diffusion upscale to improve the resolution of the simulated DeepRacer track images. Next, we will add real world features to these images through the Stable Diffusion Depth model. We will also understand the prompt engineering needed to get the expected output. These models will run on Sagemaker inference endpoints, powered by Triton inference server, through a custom execution environment created by Conda-pack.\n",
    "\n",
    "\n",
    "\n",
    "We will deploy multiple variations of Stable Diffusion on a single SageMaker Multi-Model GPU Endpoint (MME GPU) powered by NVIDIA Triton Inference Server.\n",
    "> âš  **Warning**: This notebook requires a minimum of an `ml.m5.large` instance to build the conda environment required for hosting the Stable Diffusion models.  \n",
    "\n",
    "Skip to:\n",
    "1. [Installs and imports](#installs)\n",
    "2. [Download pretrained model](#modelartifact)\n",
    "3. [Packaging a conda environment](#condaenv)\n",
    "4. [Deploy to SageMaker Real-Time Endpoint](#deploy)\n",
    "6. [Query Models to generate real world track images](#query)\n",
    "7. [Test new images with pre-trained DeepRacer model](#query)\n",
    "8. [Clean up](#cleanup)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bada6e-1e52-42b0-b658-90994df6f1a0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Architecture\n",
    "\n",
    "![](./images/lab2_arch.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbf35ff",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Part 1 - Installs and imports <a name=\"installs\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580a89c6-9054-4575-8d85-2438eabd6165",
   "metadata": {},
   "source": [
    "Let us begin by installing the python dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69df6cd4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install -U sagemaker pillow huggingface-hub conda-pack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a58d6f-6a6e-47a3-8a1b-2d6843f1536e",
   "metadata": {},
   "source": [
    "We will then initialize the sagemaker runtimes and the bucket for the future part of the session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c48876",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "import time\n",
    "import json\n",
    "from PIL import Image\n",
    "import base64\n",
    "from io import BytesIO\n",
    "import numpy as np\n",
    "\n",
    "from utils import download_model\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "# variables\n",
    "s3_client = boto3.client(\"s3\")\n",
    "ts = time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "\n",
    "# sagemaker variables\n",
    "role = get_execution_role()\n",
    "sm_client = boto3.client(service_name=\"sagemaker\")\n",
    "runtime_sm_client = boto3.client(\"sagemaker-runtime\")\n",
    "sagemaker_session = sagemaker.Session(boto_session=boto3.Session())\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = \"stable-diffusion-mme\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fda567c-afaa-4a7e-a947-98f1b15cb358",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Part 2 - Save pretrained Stable Diffusion models <a name=\"modelartifact\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30d8b41-ffc3-49c6-831f-b70e08746284",
   "metadata": {},
   "source": [
    "We will use Stable diffusion Depth and Upscaler models for modifying the DeepRacer simulated images. The `models` directory contains the inference code and the Triton configuration file for each of the Stable Diffusion models. In addition to these, we also need to download the pretrained model weights and save them to ther respective subdirectory within `models` directory. Once we have these downloaded, we can package the inference code and the model weights into a tarball and upload it to S3. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d67161-c0ce-4e62-8127-56c067d22ced",
   "metadata": {
    "tags": []
   },
   "source": [
    "We start by downloading the two Stable Diffusion models needed for this lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd50657-e4e9-4532-9763-473d1a0d9d38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "models_local_path = {\n",
    "    \"stabilityai/stable-diffusion-2-depth\": \"models/sd_depth/1/checkpoint\",\n",
    "    \"stabilityai/stable-diffusion-x4-upscaler\": \"models/sd_upscale/1/checkpoint\",\n",
    "}\n",
    "\n",
    "for model_name, model_local_path in models_local_path.items():\n",
    "    download_model(model_name, model_local_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d58e0e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Part 3 - Packaging a conda environment, extending Sagemaker Triton container <a name=\"condaenv\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8973a7d2",
   "metadata": {},
   "source": [
    "When using the Triton Python backend (which our Stable Diffusion model will run on), you can include your own environment and dependencies. The recommended way to do this is to use [conda pack](https://conda.github.io/conda-pack/) to generate a conda environment archive in `tar.gz` format, and point to it in the `config.pbtxt` file of the models that should use it, adding the snippet: \n",
    "\n",
    "```\n",
    "parameters: {\n",
    "  key: \"EXECUTION_ENV_PATH\",\n",
    "  value: {string_value: \"path_to_your_env.tar.gz\"}\n",
    "}\n",
    "\n",
    "```\n",
    "Let's start by creating the conda environment with the necessary dependencies; running these cells will output a `sd_env.tar.gz` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f523d03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile environment.yml\n",
    "name: mme_env\n",
    "dependencies:\n",
    "  - python=3.8\n",
    "  - pip\n",
    "  - pip:\n",
    "      - numpy\n",
    "      - torch\n",
    "      - accelerate\n",
    "      - transformers\n",
    "      - diffusers\n",
    "      - xformers\n",
    "      - conda-pack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b719d4ec-19a1-46cb-a887-9ccd72771e91",
   "metadata": {},
   "source": [
    "Now we can create the environment using the above environment yaml spec\n",
    "\n",
    "ðŸ›ˆ It could take up to 5 min to create the conda environment. Make sure you are running this notebook in an `ml.m5.large` instance or above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147debc0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture captured_output\n",
    "!conda env create -f environment.yml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9395fb-e0da-4e15-bf49-dfd74c1f761a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check if the environment creation was successful\n",
    "if \"Solving environment\" in captured_output.stdout:\n",
    "    print(\"Conda environment created successfully!\")\n",
    "else:\n",
    "    print(\"Conda environment creation failed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e055bfb6-f27f-4b8e-a228-d06d5003ede2",
   "metadata": {},
   "source": [
    "Next, let us package the conda environment into a tar file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3406a9fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!conda pack -n mme_env -o models/setup_conda/sd_env.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89830242-247e-458a-b2c2-6637c6845872",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Part 4 - Deploy endpoint <a name=\"deploy\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551fa2a6-3562-49aa-88bc-e35c2cf75c79",
   "metadata": {},
   "source": [
    "Now, we get the correct URI for the SageMaker Triton container image. Check out all the available Deep Learning Container images that AWS maintains [here](https://github.com/aws/deep-learning-containers/blob/master/available_images.md). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b707c2-f78e-452a-9e99-4860232bd76b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# account mapping for SageMaker Triton Image\n",
    "account_id_map = {\n",
    "    \"us-east-1\": \"785573368785\",\n",
    "    \"us-east-2\": \"007439368137\",\n",
    "    \"us-west-1\": \"710691900526\",\n",
    "    \"us-west-2\": \"301217895009\",\n",
    "    \"eu-west-1\": \"802834080501\",\n",
    "    \"eu-west-2\": \"205493899709\",\n",
    "    \"eu-west-3\": \"254080097072\",\n",
    "    \"eu-north-1\": \"601324751636\",\n",
    "    \"eu-south-1\": \"966458181534\",\n",
    "    \"eu-central-1\": \"746233611703\",\n",
    "    \"ap-east-1\": \"110948597952\",\n",
    "    \"ap-south-1\": \"763008648453\",\n",
    "    \"ap-northeast-1\": \"941853720454\",\n",
    "    \"ap-northeast-2\": \"151534178276\",\n",
    "    \"ap-southeast-1\": \"324986816169\",\n",
    "    \"ap-southeast-2\": \"355873309152\",\n",
    "    \"cn-northwest-1\": \"474822919863\",\n",
    "    \"cn-north-1\": \"472730292857\",\n",
    "    \"sa-east-1\": \"756306329178\",\n",
    "    \"ca-central-1\": \"464438896020\",\n",
    "    \"me-south-1\": \"836785723513\",\n",
    "    \"af-south-1\": \"774647643957\",\n",
    "}\n",
    "\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "if region not in account_id_map.keys():\n",
    "    raise (\"UNSUPPORTED REGION\")\n",
    "\n",
    "base = \"amazonaws.com.cn\" if region.startswith(\"cn-\") else \"amazonaws.com\"\n",
    "mme_triton_image_uri = (\n",
    "    \"{account_id}.dkr.ecr.{region}.{base}/sagemaker-tritonserver:23.03-py3\".format(\n",
    "        account_id=account_id_map[region], region=region, base=base\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c7a738",
   "metadata": {},
   "source": [
    "The next step is to package the model subdirectories and weights into individual tarballs and upload them to S3. This process can take a about 5 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3f34cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "model_root_path = Path(\"./models\")\n",
    "model_dirs = list(model_root_path.glob(\"*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db38cac8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_upload_paths = {}\n",
    "for model_path in model_dirs:\n",
    "    model_name = model_path.name\n",
    "    tar_name = model_path.name + \".tar.gz\"\n",
    "    !tar -C $model_root_path -czvf $tar_name $model_name\n",
    "    model_upload_paths[model_name] = sagemaker_session.upload_data(path=tar_name, bucket=bucket, key_prefix=prefix)\n",
    "    !rm $tar_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817c4844-d19c-4e4b-9634-63df5cd41464",
   "metadata": {},
   "source": [
    "We are now ready to configure and deploy the multi-model endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e328daa-3d5c-4c66-a6cc-7c80c4274dba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_data_url = f\"s3://{bucket}/{prefix}/\"  # s3 location where models are stored\n",
    "ts = time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "\n",
    "container = {\n",
    "    \"Image\": mme_triton_image_uri,\n",
    "    \"ModelDataUrl\": model_data_url,\n",
    "    \"Mode\": \"MultiModel\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbcc545-a421-438c-b01f-e6420a504a0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sm_model_name = f\"{prefix}-mdl-{ts}\"\n",
    "\n",
    "create_model_response = sm_client.create_model(\n",
    "    ModelName=sm_model_name, ExecutionRoleArn=role, PrimaryContainer=container\n",
    ")\n",
    "\n",
    "print(\"Model Arn: \" + create_model_response[\"ModelArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb4c11a-4f8a-409e-832e-f27b8be07bb7",
   "metadata": {},
   "source": [
    "Create a SageMaker endpoint configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c9c1a6-92de-44d6-8b0b-7c6f388da957",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoint_config_name = f\"{prefix}-epc-{ts}\"\n",
    "instance_type = \"ml.g5.2xlarge\"\n",
    "\n",
    "create_endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"InstanceType\": instance_type,\n",
    "            \"InitialVariantWeight\": 1,\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ModelName\": sm_model_name,\n",
    "            \"VariantName\": \"AllTraffic\",\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"Endpoint Config Arn: \" + create_endpoint_config_response[\"EndpointConfigArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f09a802-706e-49f4-ae36-cb4e32960b6f",
   "metadata": {},
   "source": [
    "## Create the endpoint, and wait for it to transition to `InService` state. Please do not re-run the below steps multiple times, as once endpoint gets created - we do not want to re-create it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39066acd-e31e-433d-9200-65f7bb3b6975",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoint_name = f\"{prefix}-ep-{ts}\"\n",
    "\n",
    "\n",
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "\n",
    "print(\"Endpoint Arn: \" + create_endpoint_response[\"EndpointArn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfee5c2-049d-4c96-89ea-725f99003bfe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp[\"EndpointStatus\"]\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status == \"Creating\":\n",
    "    time.sleep(30)\n",
    "    resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp[\"EndpointStatus\"]\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp[\"EndpointArn\"])\n",
    "print(\"Status: \" + status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff32048-bf34-4b27-abe7-580929bdbe39",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Part 5 - Query models <a name=\"query\"></a>\n",
    "The endpoint is now deployed and we can query the individual models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be74afb-504c-48ed-b0fc-7177774c1e03",
   "metadata": {},
   "source": [
    "Prior to invoking any of the Stable Diffusion Models, we first invoke the `setup_conda` which will copy the conda environment into a directory that can be shared with all the other models. Refer to the [model.py](./models/setup_conda/1/model.py) file in the `models/setup_conda/1` directory for more details on the implementation. Post this we will invoke the endpoint to modify the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d30696",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# invoke the setup_conda model to create the shared conda environment\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": [\n",
    "        {\n",
    "            \"name\": \"TEXT\",\n",
    "            \"shape\": [1],\n",
    "            \"datatype\": \"BYTES\",\n",
    "            \"data\": [\"hello\"],  # dummy data not used by the model\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "response = runtime_sm_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType=\"application/octet-stream\",\n",
    "    Body=json.dumps(payload),\n",
    "    TargetModel=\"setup_conda.tar.gz\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2ca25e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sample original image\n",
    "# Choose a sample image from the sample_images folder, or upload your own image to this folder and set as the SOURCE_IMAGE variable\n",
    "SOURCE_IMAGE = \"sim-image-3-00157.png\"\n",
    "\n",
    "original_image = Image.open(\"sample_images/\" + SOURCE_IMAGE)\n",
    "original_image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21b76a6-4914-4b5d-af27-fffa7198e4be",
   "metadata": {},
   "source": [
    "If the above step ran successfully, what you would see is a low-res track image as captured by Robomaker. \n",
    "\n",
    "Next , we will use a sequence of SD models -> SD Upscale and SD Depth to modify the track images and make it more aligned with a real world track."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e364fa-f7e9-4f0b-95ed-53f37a186dae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils import encode_image\n",
    "from utils import decode_image\n",
    "from PIL import Image\n",
    "import glob\n",
    "import os\n",
    "img_path = \"sample_images/\"\n",
    "output_path = \"output_images/\"\n",
    "all_files = sorted(glob.glob(img_path + '/*.png'))\n",
    "for f in all_files[:]:\n",
    "        img = Image.open(f)\n",
    "        basefile = os.path.basename(f)\n",
    "        save_output_file = output_path+\"SD_\"+basefile\n",
    "        print(save_output_file)\n",
    "        low_res_image = img.resize((128, 128))\n",
    "        inputs = dict(\n",
    "            prompt=\"Image of a racing track with border of the track as white, center line of the track as yellow, the region out of the track in green color, and outside walls are black\",\n",
    "            image=encode_image(low_res_image).decode(\"utf8\"),\n",
    "        )\n",
    "\n",
    "        payload = {\n",
    "            \"inputs\": [\n",
    "                {\"name\": name, \"shape\": [1, 1], \"datatype\": \"BYTES\", \"data\": [data]}\n",
    "                for name, data in inputs.items()\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        response = runtime_sm_client.invoke_endpoint(\n",
    "            EndpointName=endpoint_name,\n",
    "            ContentType=\"application/octet-stream\",\n",
    "            Body=json.dumps(payload),\n",
    "            TargetModel=\"sd_upscale.tar.gz\",\n",
    "        )\n",
    "        output = json.loads(response[\"Body\"].read().decode(\"utf8\"))[\"outputs\"]\n",
    "        upscaled_image = decode_image(output[0][\"data\"][0])\n",
    "\n",
    "        #### work on upscaled image\n",
    "        input_image = encode_image(upscaled_image).decode(\"utf8\")\n",
    "\n",
    "        inputs = dict(\n",
    "            prompt=\"Real world racing track with flood lights, the track should have dashed yellow center line, white track borders. White light reflections visible on the track \",\n",
    "            image=input_image,\n",
    "            gen_args=json.dumps(dict(num_inference_steps=100, strength=0.70)),\n",
    "        )\n",
    "\n",
    "\n",
    "        payload = {\n",
    "            \"inputs\": [\n",
    "                {\"name\": name, \"shape\": [1, 1], \"datatype\": \"BYTES\", \"data\": [data]}\n",
    "                for name, data in inputs.items()\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        response = runtime_sm_client.invoke_endpoint(\n",
    "            EndpointName=endpoint_name,\n",
    "            ContentType=\"application/octet-stream\",\n",
    "            Body=json.dumps(payload),\n",
    "            TargetModel=\"sd_depth.tar.gz\",\n",
    "        )\n",
    "        output = json.loads(response[\"Body\"].read().decode(\"utf8\"))[\"outputs\"]\n",
    "        modified_track = decode_image(output[0][\"data\"][0])\n",
    "        scale_down_image = encode_image(modified_track).decode(\"utf8\")\n",
    "        display(modified_track)\n",
    "\n",
    "        #Let us resize the image back to its original size thats supported by DeepRacer framework\n",
    "        modified_track = modified_track.resize((160,120))\n",
    "        modified_track = modified_track.save(save_output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cd9a38-d0eb-40ee-a475-32ba8f9162cd",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Part 6 - Validation with Pre-trained DeepRacer model <a name=\"query\"></a>\n",
    "Now, let us initiate a pre-trained model from the log analysis notebook, our aim for the remaining part of the notebook would be to pass these images to the log analysis notebook and understand where is the model looking at, when the modified track images are passed to it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c7cba8-b3d7-4c14-ae59-eea3a495d997",
   "metadata": {
    "tags": []
   },
   "source": [
    "Let us start with the imports and the installations, especially the Open CV package which provides Computer vision functions to handle images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690d6704-a955-466d-b62f-5d72ef14cb2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import boto3\n",
    "import logging\n",
    "import shutil\n",
    "import os\n",
    "import glob\n",
    "import math\n",
    "import tarfile\n",
    "import requests\n",
    "import json\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe43ceb-449f-4ca6-ad03-068d6dea07ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install shapely\n",
    "!pip install opencv-python-headless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddaa3e05-c783-4529-aa2a-c7211a7c1c84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Shapely Library\n",
    "from shapely.geometry import Point, Polygon\n",
    "from shapely.geometry.polygon import LinearRing, LineString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e4e26e-f27e-4506-bb82-bfa4d42148df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from log_analysis import *\n",
    "from os import listdir\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6aa460-4aed-48a0-bd7d-5cf90acf39dd",
   "metadata": {
    "tags": []
   },
   "source": [
    "We'll list the models you have access to in the bucket and prefix you provide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3400393c-a620-423f-9f8d-d597d179ca27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# If using your own model it needs to be imported from S3, you need to either use your DeepRacer for Cloud / Deepracer on the Spot logs uploaded to S3, \n",
    "# or export logs on the AWS DeepRacer console to S3 before this step\n",
    "BUCKET_NAME=\"\" # Enter the name of your S3 bucket where your models are stored\n",
    "PREFIX_NAME=\"\" # Enter the name of the prefix location in S3 where you model is stored, do not add a / at the end.  For console trained models choose the model name, not the time/date folder when the model was downloaded\n",
    "\n",
    "#list all folders in bucket but exclude files\n",
    "try: \n",
    "    MODELS = [obj['Prefix'][:-1] for obj in s3_client.list_objects(Bucket=BUCKET_NAME, Prefix=PREFIX_NAME+str('/'), Delimiter='/')['CommonPrefixes']]\n",
    "    i = 0\n",
    "    print(\"Models in this S3 prefix are:\")\n",
    "    while i < len(MODELS):\n",
    "        MODEL = ((MODELS[i]).split('/'))\n",
    "        print(MODEL[len(MODEL)-1])\n",
    "        i += 1\n",
    "except Exception as error:\n",
    "    print(error)\n",
    "    print(\"The specfied S3 bucket and prefix does not exist.  Only continue it you intend using one of the pre-provided models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d896649",
   "metadata": {},
   "source": [
    "Chose a model from the list above or use one of the sample models AtoZ-CCW-Centerline or AtoZ-CCW-Steering-Penalty.  Note - for console trained models you'll need to choose the folder with time/date it was downloaded to S3 (e.g. 'Fri, 23 Feb 2024 09:01:32 GMT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1619cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=\"AtoZ-CCW-Centerline\" # Change to your own model or use one of the provided examples AtoZ-CCW-Centerline or AtoZ-CCW-Steering-Penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b9bd95-18e8-499a-8957-e52684c6f15d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm -rf ./intermediate_checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4b902a-47b1-4d2a-bddd-fb7fe46c6873",
   "metadata": {
    "tags": []
   },
   "source": [
    "Let us copy the necessary model files into the lab02 path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77fa0aa-62d4-438c-b0b7-347c64bc4aee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir -p intermediate_checkpoint/model-artifacts/\n",
    "\n",
    "if model_name == \"AtoZ-CCW-Centerline\":\n",
    "    print(\"Using AtoZ-CCW-Centerline demo model\")\n",
    "    !cp -R ../deepracer_models/AtoZ-CCW-Centerline/ intermediate_checkpoint/model-artifacts/\n",
    "elif model_name == \"AtoZ-CCW-Steering-Penalty\":\n",
    "    print(\"Using AtoZ-CCW-Steering-Penalty demo model\")\n",
    "    !cp -R ../deepracer_models/AtoZ-CCW-Steering-Penalty/ intermediate_checkpoint/model-artifacts/\n",
    "else:\n",
    "    print(\"Using your own \" + model_name + \" model\")\n",
    "    try:\n",
    "        !aws s3 cp 's3://{BUCKET_NAME}/{PREFIX_NAME}/{model_name}' 'intermediate_checkpoint/model-artifacts/{model_name}' --recursive --quiet\n",
    "        !cp 'intermediate_checkpoint/model-artifacts/{model_name}/model/model_metadata.json' 'intermediate_checkpoint/model-artifacts/{model_name}/model_metadata.json'\n",
    "    except Exception as error:\n",
    "        print(\"An error has occurred, check your bucket and prefix names are correct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe1f628-127e-4710-962a-73b0c5eeb00d",
   "metadata": {
    "tags": []
   },
   "source": [
    "Next, let us read the model metadata and action space variables. These will be used in future steps while rendering the heatmap for the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9499fb25-464b-4a2b-b8cc-100a3c05a05f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"intermediate_checkpoint/model-artifacts/{}/model_metadata.json\".format(model_name),\"r\") as jsonin:\n",
    "    model_metadata=json.load(jsonin)\n",
    "sensor = [sensor for sensor in model_metadata['sensor'] if sensor != \"LIDAR\"][0]\n",
    "model_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd5925d-6ee6-4f6a-ad1f-bcee4b3d4774",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Track Segment Labels\n",
    "action_names = []\n",
    "for action in model_metadata['action_space']:\n",
    "    action_names.append(\"ST\"+str(action['steering_angle'])+\" SP\"+\"%.2f\"%action[\"speed\"])\n",
    "action_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91dba75c-c605-42d9-88dd-8a6be8cf8646",
   "metadata": {},
   "source": [
    "In the next step, we read the images created in this lab and store it in an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3084e740-80f2-40c9-90b8-0225caabc2d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "img_path = \"output_images\"\n",
    "all_files = sorted(glob.glob(img_path + '/*.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e3dce2-b530-42c6-9a80-9939eecfa769",
   "metadata": {
    "tags": []
   },
   "source": [
    "We will be using TensorFlow to run the model against the newly created images. Let us install the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd103178-1ff0-477a-a201-733548406d4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip uninstall numpy -y\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7292a5c-b48a-4fd6-bf76-99bc724a1570",
   "metadata": {
    "tags": []
   },
   "source": [
    "In the next two steps, we will import the model graph definition which is stored in protobuf format and also feed the new images to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00f9074-0857-4a20-b2b2-a24a05c6b8b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817def48-e1f6-4bdb-acce-a3bfbc2dbf4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture tensor_setup_output\n",
    "import logging\n",
    "import tensorflow.compat.v1 as tf\n",
    "from tensorflow.python.platform import gfile\n",
    "from PIL import Image\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "logger = tf.get_logger()\n",
    "logger.setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "GRAPH_PB_PATH = 'intermediate_checkpoint/'\n",
    "\n",
    "def load_session(pb_path):\n",
    "    sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True,\n",
    "                                    log_device_placement=True))\n",
    "    print(\"load graph:\", pb_path)\n",
    "    with gfile.FastGFile(pb_path,'rb') as f:\n",
    "        graph_def = tf.GraphDef()\n",
    "    graph_def.ParseFromString(f.read())\n",
    "    sess.graph.as_default()\n",
    "    tf.import_graph_def(graph_def, name='')\n",
    "    graph_nodes=[n for n in graph_def.node]\n",
    "    names = []\n",
    "    for t in graph_nodes:\n",
    "        names.append(t.name)\n",
    "\n",
    "    # For front cameras/stereo camera use the below\n",
    "    x = sess.graph.get_tensor_by_name('main_level/agent/main/online/network_0/{}/{}:0'.format(sensor, sensor))\n",
    "    y = sess.graph.get_tensor_by_name('main_level/agent/main/online/network_1/ppo_head_0/policy:0')\n",
    "\n",
    "    return sess, x, y\n",
    "\n",
    "def rgb2gray(rgb):\n",
    "    return np.dot(rgb[...,:3], [0.299, 0.587, 0.114])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78d17f7-fb17-49a8-bc7f-224f6d4b9bc2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(GRAPH_PB_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85c6020-cc63-44ad-8fc2-c5920650af8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture tensorflow_logs\n",
    "import logging\n",
    "model_inference = []\n",
    "iterations = [7,8,9]\n",
    "models_file_path = glob.glob(\"{}model-artifacts/{}/model/model*.pb\".format(GRAPH_PB_PATH, model_name))\n",
    "for model_file in models_file_path:\n",
    "\n",
    "    model, obs, model_out = load_session(model_file)\n",
    "    arr = []\n",
    "    for f in all_files[:]:\n",
    "        img = Image.open(f)\n",
    "        img_arr = np.array(img)\n",
    "        img_arr = rgb2gray(img_arr)\n",
    "        img_arr = np.expand_dims(img_arr, axis=2)\n",
    "        current_state = {\"observation\": img_arr} #(1, 120, 160, 1)\n",
    "        y_output = model.run(model_out, feed_dict={obs:[img_arr]})[0]\n",
    "        arr.append (y_output)\n",
    "    model_inference.append(arr)\n",
    "    model.close()\n",
    "    tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2118895-3214-4794-b96b-31f786982fd5",
   "metadata": {
    "tags": []
   },
   "source": [
    "We will use OpenCV methods on the Stable Diffusion generated images fed to the pre-trained DeepRacer model and overlay it to a heatmap, taking into consideration the DeepRacer model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aed1805-d43f-4393-9df1-b0b4e677cf8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def visualize_gradcam_discrete_ppo(sess, rgb_img, category_index=0, num_of_actions=5):\n",
    "    '''\n",
    "    @inp: model session, RGB Image - np array, action_index, total number of actions\n",
    "    @return: overlayed heatmap\n",
    "    '''\n",
    "\n",
    "    img_arr = np.array(img)\n",
    "    img_arr = rgb2gray(img_arr)\n",
    "    img_arr = np.expand_dims(img_arr, axis=2)\n",
    "\n",
    "    x = sess.graph.get_tensor_by_name('main_level/agent/main/online/network_0/{}/{}:0'.format(sensor, sensor))\n",
    "    y = sess.graph.get_tensor_by_name('main_level/agent/main/online/network_1/ppo_head_0/policy:0')\n",
    "    feed_dict = {x:[img_arr]}\n",
    "\n",
    "    #Get the policy head for clipped ppo in coach\n",
    "    model_out_layer = sess.graph.get_tensor_by_name('main_level/agent/main/online/network_1/ppo_head_0/policy:0')\n",
    "    loss = tf.multiply(model_out_layer, tf.one_hot([category_index], num_of_actions))\n",
    "    reduced_loss = tf.reduce_sum(loss[0])\n",
    "\n",
    "    # For front cameras use the below\n",
    "    conv_output = sess.graph.get_tensor_by_name('main_level/agent/main/online/network_1/{}/Conv2d_4/Conv2D:0'.format(sensor))\n",
    "\n",
    "    grads = tf.gradients(reduced_loss, conv_output)[0]\n",
    "    output, grads_val = sess.run([conv_output, grads], feed_dict=feed_dict)\n",
    "    weights = np.mean(grads_val, axis=(1, 2))\n",
    "    cams = np.sum(weights * output, axis=3)\n",
    "\n",
    "    ##im_h, im_w = 120, 160##\n",
    "    im_h, im_w = rgb_img.shape[:2]\n",
    "\n",
    "    cam = cams[0] #img 0\n",
    "    image = np.uint8(rgb_img[:, :, ::-1] * 255.0) # RGB -> BGR\n",
    "    cam = cv2.resize(cam, (im_w, im_h)) # zoom heatmap\n",
    "    cam = np.maximum(cam, 0) # relu clip\n",
    "    heatmap = cam / np.max(cam) # normalize\n",
    "    cam = cv2.applyColorMap(np.uint8(255 * heatmap), cv2.COLORMAP_JET) # grayscale to color\n",
    "    cam = np.float32(cam) + np.float32(image) # overlay heatmap\n",
    "    cam = 255 * cam / (np.max(cam) + 1E-5) ##  Add expsilon for stability\n",
    "    cam = np.uint8(cam)[:, :, ::-1] # to RGB\n",
    "\n",
    "    return cam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d90409a-a2d8-4616-8ac5-de2b8e7eec97",
   "metadata": {},
   "source": [
    "Now, let us loop over the Stable Diffusion generated images in the output folder and pass it to the heatmap visualization function defined above. The generated heatmaps will be stored to an array called \"heatmaps\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f8fac1-46db-4937-8f36-1b8f73bc1041",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture heatmap_cell_logs\n",
    "model_path = models_file_path[0] #Change this to your model 'pb' frozen graph file\n",
    "\n",
    "model, obs, model_out = load_session(model_path)\n",
    "heatmaps = []\n",
    "print(all_files)\n",
    "#Just need to match up the shape of the neural network\n",
    "if 'action_space_type' in model_metadata and model_metadata['action_space_type']=='continuous':\n",
    "    num_of_actions=2\n",
    "else:\n",
    "    num_of_actions=len(action_names)\n",
    "\n",
    "for f in all_files[:6]:\n",
    "    img = np.array(Image.open(f))\n",
    "    heatmap = visualize_gradcam_discrete_ppo(model, img, category_index=0, num_of_actions=num_of_actions)\n",
    "    heatmaps.append(heatmap)\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd941821-fb4b-42ca-9df5-f5c9db411d9a",
   "metadata": {
    "tags": []
   },
   "source": [
    "Finally, let us render the heatmaps to validate what the DeepRacer pre-trained model is looking at based on our newer, real world like images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf1434d-108b-48ec-be15-01efe39bde38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(len(heatmaps)):\n",
    "    plt.imshow(heatmaps[i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90862d25-33f0-4d13-9688-f789ac03fcc6",
   "metadata": {},
   "source": [
    "### This concludes our lab-02. We have successfully:\n",
    "  - Created simulated track images using Stable diffusion matching to more real world features\n",
    "  - Fed these images to a pre-trained DeepRacer model to understand what the model looks at, thereby helping the racer make decisions to modify their reward function / model.\n",
    "  \n",
    "This will help us understand how Gen AI can be used to augment and support improved model training use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e954c787-3739-41ab-9aa8-bed406cc7be9",
   "metadata": {},
   "source": [
    "Let us proceed to clean up of the Sagemaker endpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e7824e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Clean up <a name=\"query\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2688904b-426a-4d3b-8e7a-30701f0f752a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sm_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "sm_client.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "sm_client.delete_model(ModelName=sm_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a6ef61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#delete models in respective paths\n",
    "for model_name, model_local_path in models_local_path.items():\n",
    "    !rm -rf $model_local_path"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.m5.large",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
