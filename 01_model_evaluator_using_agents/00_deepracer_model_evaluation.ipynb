{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f93ddf82",
   "metadata": {},
   "source": [
    "# AWS DeepRacer Model Evaluator with Claude LLM\n",
    "\n",
    "> _This notebook should work well with the **`Data Science 3.0`** kernel in SageMaker Studio_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04f2fd3",
   "metadata": {},
   "source": [
    "This hands-on workshop demonstrates how to build a conversational agent using Amazon Bedrock with Anthropic Claude, a large language model (LLM), combined with the Langchain library. The agent is designed to provide insights and recommendations about AWS DeepRacer models and training.\n",
    "\n",
    "The workshop shows how to:\n",
    "\n",
    "* Create custom Langchain tools to allow the agent to interface with the AWS DeepRacer  service API. This includes listing available models, downloading model artifacts, and extracting model metadata like the training data and reward function.\n",
    "\n",
    "* Initialize a ReAct agent in Langchain and make the custom tools available to it. The agent can reason about which tools to invoke based on the user's questions.\n",
    "\n",
    "* Use prompting techniques like few-shot learning to improve the agent's reasoning capabilities with just a few examples.\n",
    "\n",
    "* Handle errors gracefully if the agent's responses don't match the expected format.\n",
    "\n",
    "* Leverage the custom tools to enable the agent to provide insights about an AWS DeepRacer model's training data, hyperparameters, reward function and more.\n",
    "\n",
    "By the end of the hands-on workshop, attendees will be able to build conversational agents using LLMs that can integrate with AWS services via custom interfaces. The key takeaways are being able to extend an agent's capabilities using tools, architect a modular agent, and applying prompting techniques to improve reasoning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c67be5",
   "metadata": {},
   "source": [
    "## Environment setup\n",
    "\n",
    "Before running the rest of this notebook, you'll need to install the necessary libraries and ensure there is a working connection to Amazon Bedrock and the AWS DeepRacer service API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e09649-99ba-4fb7-8304-7bf03cce5f11",
   "metadata": {},
   "source": [
    "For this lab, we will need some additional dependencies: \n",
    "- Boto3, Python SDK for AWS\n",
    "- ChromaDB, to store vector embeddings\n",
    "- Langchain,  framework for developing applications powered by language models\n",
    "- PyYAML, YAML parser\n",
    "\n",
    "⚠️ You will see pip dependency errors, you can safely ignore these errors. ⚠️\n",
    "\n",
    "IGNORE ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behavior is the source of the following dependency conflicts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005683fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install --no-build-isolation --quiet \\\n",
    "    \"boto3==1.28.62\" \\\n",
    "    \"chromadb==0.4.13\" \\\n",
    "    \"langchain==0.0.325\"  \\\n",
    "    \"pyyaml==6.0.1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45d4e99",
   "metadata": {},
   "source": [
    "### Add util libraries\n",
    "\n",
    "The util libraries contain code for interacting with the AWS DeepRacer service API, extract relevant information from DeepRacer models etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296adcde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "module_path = \"./utils\"\n",
    "sys.path.append(os.path.abspath(module_path))\n",
    "\n",
    "from utils import print_ww, deepracer, deepracer_model, s3, cloudformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fd2aac-354f-4b8e-bd03-c8bc154b00c8",
   "metadata": {},
   "source": [
    "This notebook also need access to AWS infrastructure which has been deployed into the used AWS account as part of the workshop setup. The setup of AWS infrastructure was done with Cloudformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074e4f53-81b6-4060-a09d-a548ac9365aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#If using your own AWS account please update with the name of the Cloudformation stack used to deploy the AWS infrastructure\n",
    "CLOUDFORMATION_STACK_NAME=\"sgdomainanduser\"\n",
    "\n",
    "# DEEPRACER_EXPORT_S3_BUCKET: S3 Bucket used for copying S3 models to from the DeepRacer service.\n",
    "# DEEPRACER_COPY_TO_S3_IAM_ROLE_ARN: IAM role which is passed to the DeepRacer service when copying models to the S3 bucket\n",
    "DEEPRACER_EXPORT_S3_BUCKET, DEEPRACER_COPY_TO_S3_IAM_ROLE_ARN = cloudformation.get_stack_outputs(CLOUDFORMATION_STACK_NAME)\n",
    "print(f\"DEEPRACER_EXPORT_S3_BUCKET = {DEEPRACER_EXPORT_S3_BUCKET}\")\n",
    "print(f\"DEEPRACER_COPY_TO_S3_IAM_ROLE_ARN = {DEEPRACER_COPY_TO_S3_IAM_ROLE_ARN}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7413c3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### DeepRacer service connection\n",
    "\n",
    "**Note:** The AWS DeepRacer service is only available in AWS region us-east-1. \n",
    "\n",
    "#### Verify the connection to DeepRacer\n",
    "\n",
    "Let´s check the connection to the service by listing available AWS DeepRacer models. \n",
    "\n",
    "First enter your racer name, replacing the example 'superfast_racer' below.  This can be found in 'Your racer profile' section of the DeepRacer Console"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56bc16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "RACER_PROFILE_NAME = \"superfast_racer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ab71d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "models = deepracer.list_models()\n",
    "\n",
    "for model in models:\n",
    "    if model['CreatedByAlias']==RACER_PROFILE_NAME: \n",
    "        print(f\"Name: {model['ModelName']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e63f252",
   "metadata": {},
   "source": [
    "This call will return the names of the DeepRacer models in your account, to have a conversation with the agent about the models they should have either been created in the console or imported and then evaluated in the console."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a441225",
   "metadata": {},
   "source": [
    "### Bedrock service connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ccf10d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "bedrock_client = boto3.client('bedrock-runtime')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cfc903",
   "metadata": {},
   "source": [
    "### LLM setup\n",
    "\n",
    "In this lab we will use the following LLM:s\n",
    "- Claude instant v1, for the reasoning and act (ReAct) agent as well as for building a DeepRacer Question & Answer LLM tool. \n",
    "- Amazon Titan embeddings model, to allow the DeepRacer Question & Answer LLM tool to do semantic search on the DeepRacer documentation to improve the response quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa850cca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms.bedrock import Bedrock\n",
    "from langchain.embeddings import BedrockEmbeddings\n",
    "\n",
    "llm_claude_instant = Bedrock(\n",
    "    model_id=\"anthropic.claude-instant-v1\",\n",
    "    client=bedrock_client,\n",
    "    model_kwargs={ \"temperature\": 0, \"max_tokens_to_sample\": 3000},\n",
    ")\n",
    "\n",
    "bedrock_embeddings_client = BedrockEmbeddings(\n",
    "        client=bedrock_client,\n",
    "        model_id=\"amazon.titan-embed-text-v1\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384666fd",
   "metadata": {},
   "source": [
    "## Provide more upto date AWS DeepRacer knowledge to the LLM\n",
    "\n",
    "Claude instant has previous knowledge of AWS DeepRacer but the knowledge is not up to date with the latest version of the AWS DeepRacer developer guidelines. To make the model aware of more recent additions we can create an Langchain chain which utilize Retrieval Augmentation Generation (RAG). \n",
    "\n",
    "We will not cover how RAG works in detail in this workshop, for more information:\n",
    "- [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401)\n",
    "- [Bedrock workshop](https://github.com/aws-samples/amazon-bedrock-workshop/tree/main/03_QuestionAnswering)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503b451b",
   "metadata": {},
   "source": [
    "### AWS DeepRacer knowledge base tool\n",
    "\n",
    "This tool is another Langchain chain specifically designed to answer AWS DeepRacer related questions. This chain use Retrieval Augmentation Generation (RAG) to get documents relevant to the asked question to provide a better foundation to answer AWS DeepRacer related questions.\n",
    "\n",
    "The AWS DeepRacer developer guideline was loaded and split into chunks and converted to embeddings.\n",
    "\n",
    "![](./images/Embeddings_lang.png)\n",
    "\n",
    "Embeddings are vector representations of words that encode semantic meaning. In retrieval augmented generation, embeddings help find relevant external knowledge to augment the capabilities of a generative chatbot LLM. The chatbot LLM uses the retrieved embeddings during generation to construct more knowledgeable and relevant response.\n",
    "\n",
    "![](./images/Chatbot_lang.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a291d49",
   "metadata": {},
   "source": [
    "#### Setup the vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a491ef1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# Load the pre-made embeddings into the Vector store.\n",
    "vectorstore_chromadb = Chroma(persist_directory=\"./persistent/chroma_db\", embedding_function=bedrock_embeddings_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51a2f44",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Create the AWS DeepRacer QA chain\n",
    "\n",
    "For building the RAG chain we utilize one of the Langchain chains, [Retrieval QA chain](https://js.langchain.com/docs/modules/chains/popular/vector_db_qa/), which has built in support for RAG.\n",
    "\n",
    "The prompt is using a couple of techniques to ensure AWS DeepRacer relevant responses:\n",
    "- The documents retrieved from the vector store is inserted into the {context} variable to make them available to the LLM\n",
    "- Reduce hallucinations by requiring the LLM to use the information retrieved via RAG to answer the question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67693d60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = \"\"\"Human: Given an AWS DeepRacer related question, provide a detailed answer.\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Here is the human's next reply:\n",
    "<human_reply>\n",
    " {question}\n",
    "</human_reply>\n",
    "\n",
    "If the answer cannot be found in the context, the AI truthfully says it does not know the answer.\n",
    "\n",
    "Assistant:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "deepracer_qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm_claude_instant,\n",
    "    chain_type=\"stuff\",  # add all returned documents\n",
    "    retriever=vectorstore_chromadb.as_retriever(\n",
    "        search_type=\"similarity\",\n",
    "        search_kwargs={\n",
    "            \"k\": 3\n",
    "        },  # return the top 3 similar documents from the vector store\n",
    "    ),\n",
    "    # return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": prompt},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48de31e4",
   "metadata": {},
   "source": [
    "#### Create a Langchain custom tool\n",
    "\n",
    "Now we package the AWS DeepRacer QA chain as Langchain custom tool and make it available to the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a478de9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.agents import Tool\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Initialize the LLM tool\n",
    "deepracer_knowledge_tool = Tool.from_function(\n",
    "    name='AWS DeepRacer knowledge base',\n",
    "    func=deepracer_qa_chain.run,\n",
    "    description='Useful when you need to answer generic questions about AWS DeepRacer, Input: the question to answer, Output: Answer to the question',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50957832",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tools = [\n",
    "        deepracer_knowledge_tool,\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6b4b15",
   "metadata": {},
   "source": [
    "## Using ReAct: Synergizing Reasoning and Acting in Language Models Framework\n",
    "Large language models can generate both explanations for their reasoning and task-specific responses in an alternating fashion.\n",
    "\n",
    "Producing reasoning explanations enables the model to infer, monitor, and revise action plans, and even handle unexpected scenarios. The action step allows the model to interface with and obtain information from external sources such as knowledge bases or environments.\n",
    "\n",
    "Here we will initialize the chain responsible for deciding what step to take next. This is powered by a language model and a prompt. The inputs to this chain are:\n",
    "\n",
    "- List of available tools\n",
    "- User input\n",
    "- Any previously executed steps (intermediate_steps)\n",
    "\n",
    "This chain then returns either the next action to take or the final response to send to the user (AgentAction or AgentFinish).\n",
    "\n",
    "Different [Langchain agent types](https://python.langchain.com/docs/modules/agents/agent_types/) have different prompting styles for reasoning, different ways of encoding input, and different ways of parsing the output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991ed4ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent, AgentType\n",
    "\n",
    "react_agent = initialize_agent(tools,\n",
    "                               llm=llm_claude_instant,\n",
    "                               agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "                               verbose=True, # displays the intermediate steps the agent performs\n",
    "                               return_intermediate_steps=True,\n",
    "                               #handle_parsing_errors=True,\n",
    "                               )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a46ef2",
   "metadata": {},
   "source": [
    "Let´s have a look at the default prompt used for this chain.\n",
    "\n",
    "Please note that the prompt template has the following structure:\n",
    "```\n",
    "\n",
    "Answer the following questions as best you can. You have access to the following tools:\n",
    "\n",
    "{available tools}\n",
    "\n",
    "Use the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action: the action to take, should be one of {tool names}\n",
    "Action Input: the input to the action\\nObservation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Begin!\n",
    "\n",
    "Question: {input}\n",
    "Thought: {agent_scratchpad}\n",
    "\n",
    "```\n",
    "\n",
    "The default prompt instructs the model how to reason and interact with the available tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfa0907",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "react_agent.agent.llm_chain.prompt.template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b711eec1",
   "metadata": {},
   "source": [
    "Once the agent ```react_agent``` is setup we can invoke the agent by passing a prompt/question as seen blow. Feel free to adjust the prompt/question on subsequent calls.\n",
    "\n",
    "When running the below command you should see how the agent starts to reason what it need to do in order to answer the question, what tools is uses and their input. \n",
    "If all goes well the agent should produce a final answer.\n",
    "\n",
    "The question below the agent usually manage to resolve in one plan->act->observe loop and you should see a \"> Finished chain.\" with the final answer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚠️ **If you get an access denied issue when running the below cell, please check in Amazon Bedrock that both, Claude Instant and Titan Embeddings G1 – Text, has a status of Access granted.** ⚠️\n",
    "\n",
    "⚠️ **If you get an OutputParserException. This can happen sometimes due to that the LLM do not return the response to the question in the correct format. This we will improve later on in the lab by introducing a few-shot prompt, provide some examples in the prompt on how the LLM should respond** ⚠️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c237474",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "question = \"What is AWS DeepRacer?\"\n",
    "print_ww(react_agent(question)[\"output\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a4bc4b-fc9c-4012-9596-0078f97b6471",
   "metadata": {},
   "source": [
    "This question showcase one issue that can happen when using agents. When resolving this question the agent usually ends up in a loop with the same thought and action each time until it can´t follow the format it has been instructed to.\n",
    "\n",
    "This we will improve later on in the lab by introducing a few-shot prompt, provide some examples in the prompt on how the LLM should reason, and have the agent manage parsing errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9d7514",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    question = \"How do I get started with training a DeepRacer model? anything specific I should think of?\"\n",
    "    print_ww(react_agent(question)[\"output\"])\n",
    "except Exception as e:\n",
    "    print(f\"\\n\\nThe agent encountered an error! \\n {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd19396",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Add more tools to improve the LLMs capability to analyze DeepRacer models\n",
    "\n",
    "So far the LLM can only answer generic information about AWS DeepRacer which is part of the [AWS DeepRacer developer guide](https://docs.aws.amazon.com/deepracer/latest/developerguide/what-is-deepracer.html). \n",
    "In this section we will integrate the LLM with the AWS DeepRacer service API so the LLM can interact with different models you have access to in this AWS account."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b9b033",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create a tool to list AWS DeepRacer models\n",
    "\n",
    "When invoked by the agent this tool will list available DeepRacer models located in the AWS DeepRacer service. It has been implemented as a Langchain custom tool and use custom code to call the DeepRacer service API when invoked by the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a76ea86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.tools import BaseTool\n",
    "\n",
    "\n",
    "class DeepRacerListModelsTool(BaseTool):\n",
    "    name = \"List AWS DeepRacer models\"\n",
    "    description = \"\"\"\n",
    "    Use this tool when you need to list all AWS DeepRacer models. Contains information on who created the model, model name, description.\n",
    "    \"\"\"\n",
    "\n",
    "    def _run(self, input=None):\n",
    "        # List all available DeepRacer models\n",
    "        models = deepracer.list_models()\n",
    "\n",
    "        # Extract only relevant attributes from the model objects to make it easier for the ReAct LLM to answer our questions\n",
    "        models_found = []\n",
    "        for model in models:\n",
    "            if model['CreatedByAlias']==RACER_PROFILE_NAME: \n",
    "                models_found.append(\n",
    "                    {\n",
    "                        key: model[key]\n",
    "                        for key in model.keys()\n",
    "                        & {\n",
    "                            \"ModelName\",\n",
    "                            \"ModelDescription\",\n",
    "                            \"CreatedByAlias\",\n",
    "                            \"CarConfiguration\",\n",
    "                        }\n",
    "                    }\n",
    "                )\n",
    "        return models_found\n",
    "\n",
    "    def _arun(self, radius: int):\n",
    "        raise NotImplementedError(\"This tool does not support async\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca56892",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create a tool to get AWS DeepRacer model details.\n",
    "\n",
    "Downloads an AWS DeepRacer model and extract relevant parts to allow the LLM to do an analysis of its training and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff719b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from langchain.tools import BaseTool\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class ModelAnalysisInput(BaseModel):\n",
    "    model_name: str = Field()\n",
    "\n",
    "\n",
    "class DeepRacerModelAnalysisTool(BaseTool):\n",
    "    name = \"AWS DeepRacer model details\"\n",
    "    description = \"\"\"Use this tool to get detailed information about an AWS DeepRacer model. Input: Expects the model_name as string input. Output: Python dictionary\"\"\"\n",
    "    args_schema = ModelAnalysisInput\n",
    "\n",
    "    def _run(self, model_name=None):\n",
    "        try:\n",
    "            # Copy the DeepRacer model from AWS DeepRacer service to a S3 bucket.\n",
    "            formatted_model_name = model_name.strip()\n",
    "            target_s3_bucket = DEEPRACER_EXPORT_S3_BUCKET\n",
    "            model_s3_prefix = deepracer.copy_model_to_s3_if_model_does_not_exist(\n",
    "                formatted_model_name,\n",
    "                target_s3_bucket,\n",
    "                DEEPRACER_COPY_TO_S3_IAM_ROLE_ARN,\n",
    "            )\n",
    "\n",
    "            # Extract relevant information from the downloaded model files.\n",
    "            model = deepracer_model.DeepRacerModel(target_s3_bucket, model_s3_prefix)\n",
    "            model_data = {}\n",
    "            model_data[\"model_metadata_used_for_training\"] = model.get_model_meta_data()\n",
    "            model_data[\n",
    "                \"reward_function_used_for_training\"\n",
    "            ] = model.get_reward_function()\n",
    "            model_data[\n",
    "                \"hyper_parameters_used_for_training\"\n",
    "            ] = model.get_hyper_parameters()\n",
    "            model_data[\"evaluation_results\"] = model.get_evaluation_metrics()\n",
    "            model_data[\"training_results\"] = model.get_training_metrics()\n",
    "            model_data[\"track_meta_data\"] = model.get_track_meta_data()\n",
    "            return model_data\n",
    "        except FileNotFoundError as e:\n",
    "            return f\"Model with name {model_name} does not exist, {e}\"\n",
    "\n",
    "    def _arun(self, radius: int):\n",
    "        raise NotImplementedError(\"This tool does not support async\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d55a075",
   "metadata": {},
   "source": [
    "After having created our third and final tool, we add the 3 tools to a list that will be passed, as a positional argument, to the agent initialization function. Making it possible for the agent to use them when resolving prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57058eb7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tools = [\n",
    "        DeepRacerModelAnalysisTool(),\n",
    "        DeepRacerListModelsTool(),\n",
    "        deepracer_knowledge_tool,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b022844c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent, AgentType\n",
    "\n",
    "react_agent = initialize_agent(tools,\n",
    "                               llm=llm_claude_instant,\n",
    "                               agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "                               verbose=True, # verbose logs is turned on to give insights into how the agent will reason and use the tools\n",
    "                               return_intermediate_steps=True,\n",
    "                               handle_parsing_errors=True,\n",
    "                               )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38334d93",
   "metadata": {},
   "source": [
    "Before testing out the newly added tools let´s also improve the agents ability to reason, observe and act in the required format.\n",
    "\n",
    "One way to do this is to introduce a few-shot prompt, eg: provide a couple of reasoning examples.\n",
    "In the below prompt we have also adapted the default prompt to include ```/n/nHuman``` and ```/n/nAssistant```. Claude is trained to fill in text for the Assistant role as part of an ongoing dialogue between a human user (Human:) and an AI assistant (Assistant:).\n",
    "\n",
    "For more details on prompting with Anthropic Claude see the [Anthropic prompting guide](https://docs.anthropic.com/claude/docs/introduction-to-prompt-design)\n",
    "\n",
    "Another way is to enable ```handle_parsing_errors=True``` as we have done in the above cells. Doing this the agent will try to remediate the error by it self.\n",
    "\n",
    "```\n",
    "Thought: This gives a good overview but does not provide all the necessary details. I should check the model details next.\n",
    "\n",
    "Action: AWS DeepRacer model details\n",
    "\n",
    "Observation: Invalid Format: Missing 'Action Input:' after 'Action:'\n",
    "Thought: Checking the model details did not provide the expected response. Let me try listing the available models to get more context.\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b72fca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "prompt_template = \"\"\"Human: Use the following format:\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do, Also try to follow steps mentioned above.\n",
    "Action: the action to take, should be one of {tools}.\n",
    "Action Input: the input to the action.\n",
    "Observation: the result of the action.\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: I now know the final answer.\n",
    "Final Answer: the final answer to the original input question.\n",
    "\n",
    "<examples>\n",
    "<example>\n",
    "Question: What is AWS DeepRacer?\n",
    "Thought: I need to get more information on what AWS DeepRacer is, I should check the AWS DeepRacer knowledge base\n",
    "Action: AWS DeepRacer knowledge base\n",
    "Action Input: What is AWS DeepRacer?\n",
    "Observation:  AWS DeepRacer is the fastest way to get rolling with reinforcement learning (RL), literally, with a fully autonomous 1/18th scale race car driven by reinforcement learning, 3D racing simulator, and a global racing league.\n",
    "Final Answer: AWS DeepRacer is the fastest way to get rolling with reinforcement learning (RL), literally, with a fully autonomous 1/18th scale race car driven by reinforcement learning, 3D racing simulator, and a global racing league.\n",
    "</example>\n",
    "<example>\n",
    "Question: how can I improve my-deepracer-model?\n",
    "Thought: I need to check the details and performance of the AWS DeepRacer model to answer this question.\n",
    "Action: AWS DeepRacer model details\n",
    "Action Input: my-deepracer-model\n",
    "Observation: ....\n",
    "Thought: I should check the AWS DeepRacer knowledge base on how to improve a DeepRacer model\n",
    "Action: AWS DeepRacer knowledge base\n",
    "Action Input: Improve deepracer model\n",
    "Observation: ....\n",
    "Thought: With the details about the model and knowledge on how to improve a model I know have enough information.\n",
    "Final Answer: The my-deepracer-model model is designed and trained to follow the center line of the track. The model details, reward function, and hyperparameters used for training provide evidence that this model prioritizes staying on the center line, making it a good performing model for that task.\n",
    "</example>\n",
    "</examples>\n",
    "\n",
    "You are an AWS DeepRacer Q&A bot, give a detailed answer to the question.\n",
    "You can only answer question not perform any changes to any DeepRacer models.\n",
    "Question: {input}\n",
    "\n",
    "\n",
    "Assistant:\n",
    "{agent_scratchpad}\"\"\"\n",
    "\n",
    "# Create a prompt template and inject the names of all available tools in the {tools} variable. This can be done using partial_variables\n",
    "tool_names = json.dumps([d.name for d in tools])\n",
    "template = PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=[\"input\", \"agent_scratchpad\"],\n",
    "    partial_variables={\"tools\": tool_names},\n",
    ")\n",
    "react_agent.agent.llm_chain.prompt = template\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c100d3b9",
   "metadata": {},
   "source": [
    "With the addition of DeepRacerListModelsTool and DeepRacerModelAnalysisTool custom tools the agent is now able to:\n",
    "1. List available AWS DeepRacer models\n",
    "2. Download a named DeepRacer model and extract: \n",
    "     - training data\n",
    "     - evaluation data\n",
    "     - extract model meta data, like action space, hyperparameters etc\n",
    "     - extract the reward function\n",
    "\n",
    "As we say earlier in the lab the agent has access to some DeepRacer models in this AWS account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    if model['CreatedByAlias']==RACER_PROFILE_NAME: \n",
    "        print(f\"Name: {model['ModelName']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These DeepRacer model names can be used when asking questions to the agent. \n",
    "\n",
    "In this question the agent will use the **List AWS DeepRacer models** tool to via the DeepRacer service API get the names of the DeepRacer models in the current AWS account. After the **Finished chain** is the output that is returned to the user. \n",
    "\n",
    "The text that is seen before the **Finished chain** is just logs produced due to that we have **verbose=True** turned on for the LLM used by the agent. This is to better highlight how the agent is reasoning and interact with the tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e3ecef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "question = \"What are the names of DeepRacer models I have access to?\"\n",
    "print_ww(react_agent(question)[\"output\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this question the agent will use the **AWS DeepRacer model details** tool to via the DeepRacer service API extract **MODEL_NAME** model details (hyperparameters, reward function, training metrics, evaluation metrics and track used). This data is then returned to the agent as json and the agent will search for which track was used for the evaluation\n",
    "\n",
    "After the **Finished chain** is the output that is returned to the user. \n",
    "\n",
    "Change the MODEL_NAME variable to the name of the model you want to learn about"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a4f65b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_NAME=\"AtoZ-CCW-Centerline\"\n",
    "COMPARE_MODEL_NAME=\"AtoZ-CCW-Steering-Penalty\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a4f65b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "question = \"Which track was \" + MODEL_NAME + \" trained and evaluated on?\"\n",
    "print_ww(react_agent( question)[\"output\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let´s try some more questions to see how the agent reason and use the tools. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7e9047",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "question = \"Is \" + MODEL_NAME + \" or \" + COMPARE_MODEL_NAME + \" the better model? please also state why\"\n",
    "print_ww(react_agent( question)[\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34447e1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "question = \"How can I improve the training for \" + MODEL_NAME + \"?\"\n",
    "print_ww(react_agent( question)[\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028d8cc4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "question = \"Should I change any of the hyperparameters that was used to train \" + MODEL_NAME + \" to improve the training?\"\n",
    "print_ww(react_agent( question)[\"output\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71a4bb7-e42f-438a-99ec-6fc589cde72f",
   "metadata": {},
   "source": [
    "# Clean up\n",
    "\n",
    "To avoid incurring charges let´s delete the downloaded models from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c567d1-e7c5-4ea5-9feb-07da266669ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3.delete_s3_prefix(DEEPRACER_EXPORT_S3_BUCKET, \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7d0c68",
   "metadata": {},
   "source": [
    "# Recap\n",
    "\n",
    "In this lab we integrated a Langchain agent with the AWS DeepRacer service using a custom tool. This allowed the LLM to help us with model analysis and how to improve them.\n",
    "\n",
    "\n",
    "### Next steps:\n",
    "[Amazon Bedrock workshop](https://github.com/aws-samples/amazon-bedrock-workshop) - More use cases and ways to build with Amazon Bedrock\n",
    "\n",
    "[SageMaker JumpStart](https://docs.aws.amazon.com/sagemaker/latest/dg/studio-jumpstart.html) - Provides pre-trained, open-source models for a wide range of problem types to help you get started with machine learning.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
